\documentclass[11pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{color}

% --- Page Layout ---
\geometry{left=2.5cm, right=2.5cm, top=3cm, bottom=3cm}

% --- Author & Metadata ---
\title{\textbf{Efficient Manifold-Constrained Hyper-Connections via \\ Smooth Algebraic Parametrization}}

\author{
  \textbf{Tetsu Yamaguchi} \\
  Knowgic Technology \\
  \texttt{tetsuy@knowgictech.com} \\
  \vspace{0.2cm} \\
  \textbf{GitHub:} \url{https://github.com/ty-knowgic/stabilized-mhc}
}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
\noindent
Hyper-Connections (HC) and its manifold-constrained variant (mHC) are critical architectural components in recent Large Language Models, such as DeepSeek-V3. Current implementations rely on the iterative Sinkhorn-Knopp algorithm to approximate doubly stochastic matrices, which introduces significant computational overhead and memory bandwidth saturation.

In this technical report, we propose \textbf{Stabilized Piecewise-Rational Charts (SPRC)}, a constructive algebraic method that parameterizes the Birkhoff polytope for $n=4$ without iteration. By employing a smooth tropical norm (LogSumExp) and progressive saturation ($\tanh$), our method guarantees exact constraint satisfaction and differentiability. Benchmarks on NVIDIA T4 GPUs demonstrate a \textbf{13.3$\times$ speedup} in kernel execution and a \textbf{2.53$\times$ reduction} in end-to-end training time compared to the Sinkhorn baseline ($t_{max}=20$), while achieving equivalent convergence properties.
\end{abstract}

\section{Introduction}
Recent work by DeepSeek-AI \cite{deepseek2024} introduced Manifold-Constrained Hyper-Connections (mHC) to restore the identity mapping property in expanded residual streams. While effective, the reliance on the Sinkhorn-Knopp algorithm ($t_{max}=20$) for manifold projection imposes a "memory wall" bottleneck. We present an algebraic parametrization that eliminates these loops, achieving theoretical lower-bound latency while maintaining the representational capacity of the layer.

\section{Methodology: Smooth Algebraic Parametrization}
Unlike Sinkhorn, which solves an optimization problem, we construct a smooth mapping from the parameter space $\mathbb{R}^9$ to the relative interior of the Birkhoff polytope $\mathcal{B}_4$.

\subsection{Tangent Space Construction}
Let $u \in \mathbb{R}^9$. We map $u$ to a direction vector $V \in \mathbb{R}^{4 \times 4}$ in the tangent space (zero-sum subspace) via a sparse linear transform $\mathcal{L}$:
\begin{equation}
    V = \mathcal{L}(u), \quad \text{s.t.} \quad \sum_j V_{ij} = 0, \quad \sum_i V_{ij} = 0
\end{equation}

\subsection{Progressive Saturation and Smoothness}
To ensure the output lies strictly within the polytope and maintains differentiability, we employ a **Smooth Tropical Norm** using the LogSumExp (LSE) function to approximate the distance to the boundary:
\begin{equation}
    m_{smooth}(V) = \eta \log \left( \sum_{i,j} \exp\left(\frac{-V_{ij}}{\eta}\right) \right)
\end{equation}
Crucially, to allow the model to approach the boundary (identity-like permutations) as parameters grow, we utilize a **Progressive Saturation** function based on the hyperbolic tangent. The final doubly stochastic matrix $H$ is given by:
\begin{equation}
    H(u) = J_4 + \tanh(\lambda \|V\|_F) \frac{0.25 \cdot V}{m_{smooth}(V) + \epsilon}
\end{equation}
where $J_4$ is the center of the polytope ($J_{ij}=0.25$). This formulation ensures that as $\|V\| \to \infty$, the output approaches the boundary faces, preserving the expressive power required for mHC.

\subsection{Proof of Exactness}
We rigorously prove that $H(u)$ is a doubly stochastic matrix.

\paragraph{Sum Constraint.}
Since $V$ has zero row/column sums, any scaling of $V$ added to $J_4$ preserves the sum:
\begin{equation}
    \sum_j H_{ij} = \sum_j 0.25 + \beta \sum_j V_{ij} = 1 + 0 = 1
\end{equation}
This holds strictly by construction.

\paragraph{Non-Negativity Constraint.}
The LogSumExp function provides a smooth upper bound on the maximum element: $m_{smooth}(V) \ge \max_{i,j}(-V_{ij})$. 
Let $v_{min} = \min_{i,j} V_{ij}$ be the most negative element of $V$. Then $-v_{min} = \max_{i,j}(-V_{ij}) \le m_{smooth}(V)$.
For any entry $H_{ij}$, the worst case occurs when $V_{ij}$ is negative:
\begin{equation}
    H_{ij} \ge 0.25 - 0.25 \cdot \underbrace{\tanh(\dots)}_{\le 1} \cdot \frac{-V_{ij}}{m_{smooth}(V) + \epsilon}
\end{equation}
Since $-V_{ij} \le m_{smooth}(V)$, the fraction is always $< 1$. Thus $H_{ij} > 0$ is strictly guaranteed for $\epsilon > 0$.

\section{Experimental Results}
We validated the method against the Sinkhorn baseline ($t_{max}=20$) on an NVIDIA T4 GPU.

\subsection{Computational Efficiency}
Our method eliminates kernel launch latency and memory bottlenecks, achieving massive speedups in kernel benchmarks (Table \ref{tab:speedup}).

\begin{table}[h]
\centering
\caption{Forward+Backward Execution Time (ms)}
\label{tab:speedup}
\begin{tabular}{rccc}
\toprule
Batch Size & Sinkhorn & \textbf{Ours (SPRC)} & Speedup \\
\midrule
1,024  & 5.13 & \textbf{0.98} & $5.2\times$ \\
65,536 & 15.43 & \textbf{1.16} & $\mathbf{13.3\times}$ \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Analysis via Amdahl's Law.}
The discrepancy between the kernel speedup ($13.3\times$) and the end-to-end training speedup ($2.53\times$) is consistent with Amdahl's Law. In our training loop, fixed costs such as Linear layers, LayerNorm, and optimizer steps consume approximately 60\% of the baseline iteration time. By reducing the mHC cost to near-zero ($O(1)$), we achieve the theoretical maximum acceleration for the entire system.

\subsection{Learning Dynamics}
We trained a toy Transformer layer on an auto-regressive task. The proposed method matches the convergence profile of Sinkhorn almost perfectly (Figure \ref{fig:loss}), with a final loss difference of $<0.1\%$, while training \textbf{2.53$\times$} faster end-to-end.

\begin{figure}[h]
\centering
% Save your colab plot as loss_plot.png
\includegraphics[width=0.8\textwidth]{loss_plot3.png}
\caption{Training Loss Convergence. Our method (Orange) tracks the Sinkhorn baseline (Blue) closely, demonstrating that the algebraic parametrization preserves the layer's expressivity.}
\label{fig:loss}
\end{figure}

\section{Conclusion}
We proposed a smooth algebraic parametrization for mHC layers that replaces iterative approximation with an exact, differentiable closed-form solution. The method achieves a 13.3$\times$ kernel speedup and practically identical convergence behavior, offering a superior alternative for large-scale model training.

\begin{thebibliography}{9}
\bibitem{deepseek2024}
Z. Xie et al., "Manifold-Constrained Hyper-Connections," arXiv:2512.24880, 2026.
\end{thebibliography}

\end{document}